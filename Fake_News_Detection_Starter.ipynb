{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhR-ZUkwJrFn"
   },
   "source": [
    "# Fake News Detection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXeHUV5fJGiZ"
   },
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX73YZdgJIgF"
   },
   "source": [
    "The objective of this assignment is to develop a Semantic Classification model. You will be using Word2Vec method to extract the semantic relations from the text and develop a basic understanding of how to train supervised models to categorise text based on its meaning, rather than just syntax. You will explore how this technique is used in situations where understanding textual meaning plays a critical role in making accurate and efficient decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gg_J6K8Xxfk2"
   },
   "source": [
    "## Business Objective\n",
    "\n",
    "The spread of fake news has become a significant challenge in today’s digital world. With the massive volume of news articles published daily, it’s becoming harder to distinguish between credible and misleading information. This creates a need for systems that can automatically classify news articles as true or fake, helping to reduce misinformation and protect public trust.\n",
    "\n",
    "\n",
    "In this assignment, you will develop a Semantic Classification model that uses the Word2Vec method to detect recurring patterns and themes in news articles. Using supervised learning models, the goal is to build a system that classifies news articles as either fake or true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySqxOckxI4-F"
   },
   "source": [
    "<h2> Pipelines that needs to be performed </h2>\n",
    "\n",
    "You need to perform the following tasks to complete the assignment:\n",
    "\n",
    "<ol type=\"1\">\n",
    "\n",
    "  <li> Data Preparation\n",
    "  <li> Text Preprocessing\n",
    "  <li> Train Validation Split\n",
    "  <li> EDA on Training Data\n",
    "  <li> EDA on Validation Data [Optional]\n",
    "  <li> Feature Extraction\n",
    "  <li> Model Training and Evaluation\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTxV-3GJUhWm"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofebI8ITG-Li"
   },
   "source": [
    "**NOTE:** The marks given along with headings and sub-headings are cumulative marks for those particular headings/sub-headings.<br>\n",
    "\n",
    "The actual marks for each task are specified within the tasks themselves.\n",
    "\n",
    "For example, marks given with heading *2* or sub-heading *2.1* are the cumulative marks, for your reference only. <br>\n",
    "\n",
    "The marks you will receive for completing tasks are given with the tasks.\n",
    "\n",
    "Suppose the marks for two tasks are: 3 marks for 2.1.1 and 2 marks for 3.2.2, or\n",
    "* 2.1.1 [3 marks]\n",
    "* 3.2.2 [2 marks]\n",
    "\n",
    "then, you will earn 3 marks for completing task 2.1.1 and 2 marks for completing task 3.2.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdQjht7dUiHt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b1lNTpKF54T"
   },
   "source": [
    "## Data Dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43j_A_GsI9TS"
   },
   "source": [
    "For this assignment, you will work with two datasets, `True.csv` and `Fake.csv`.\n",
    "Both datasets contain three columns:\n",
    "<ul>\n",
    "  <li> title of the news article\n",
    "  <li> text of the news article\n",
    "  <li> date of article publication\n",
    "</ul>\n",
    "\n",
    "`True.csv` dataset includes 21,417 true news, while the `Fake.csv` dataset comprises 23,502 fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oTQwQ_Rh4nT"
   },
   "source": [
    "## Installing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lIY57QOLiCA2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade numpy==1.26.4\n",
    "######!pip install --upgrade pandas==2.2.2\n",
    "#####!pip install --upgrade nltk==3.9.1\n",
    "####!pip install --upgrade spacy==3.7.5\n",
    "###!pip install --upgrade scipy==1.12\n",
    "##!pip install --upgrade pydantic==2.10.5\n",
    "#!pip install wordcloud==1.9.4\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuLFIymAL58u"
   },
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O-Q9pqrcJrFr"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation and analysis\n",
    "import numpy as np  # For numerical operations and arrays\n",
    "import pandas as pd  # For working with dataframes and structured data\n",
    "import re  # For regular expression operations (text processing)\n",
    "import nltk  # Natural Language Toolkit for text processing\n",
    "import spacy  # For advanced NLP tasks\n",
    "import string  # For handling string-related operations\n",
    "\n",
    "# Optional: Uncomment the line below to enable GPU support for spaCy (if you have a compatible GPU)\n",
    "#spacy.require_gpu()\n",
    "\n",
    "# Load the spaCy small English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# For data visualization\n",
    "import seaborn as sns  # Data visualization library for statistical graphics\n",
    "import matplotlib.pyplot as plt  # Matplotlib for creating static plots\n",
    "# Configure Matplotlib to display plots inline in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress unnecessary warnings to keep output clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For interactive plots\n",
    "from plotly.offline import plot  # Enables offline plotting with Plotly\n",
    "import plotly.graph_objects as go  # For creating customizable Plotly plots\n",
    "import plotly.express as px  # A high-level interface for Plotly\n",
    "\n",
    "# For preprocessing and feature extraction in machine learning\n",
    "from sklearn.feature_extraction.text import (  # Methods for text vectorization\n",
    "    CountVectorizer,  # Converts text into a bag-of-words model\n",
    ")\n",
    "\n",
    "# Import accuracy, precision, recall, f_score from sklearn to predict train accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Pretty printing for better readability of output\n",
    "from pprint import pprint\n",
    "\n",
    "# For progress tracking in loops (useful for larger datasets)\n",
    "from tqdm import tqdm, tqdm_notebook  # Progress bar for loops\n",
    "tqdm.pandas()  # Enables progress bars for pandas operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s8Le3OfjI666"
   },
   "outputs": [],
   "source": [
    "## Change the display properties of pandas to max\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtRLCsNVJrFt"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Load the True.csv and Fake.csv files as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "puVzIf_iJrFt"
   },
   "outputs": [],
   "source": [
    "# Import the first file - True.csv\n",
    "True_News = pd.read_csv(\"True.csv\")\n",
    "# Import the second file - Fake.csv\n",
    "Fake_News = pd.read_csv(\"Fake.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xYpH-sAJrFu"
   },
   "source": [
    "## **1.** Data Preparation  <font color = red>[10 marks]</font>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2fTeYJImEv7"
   },
   "source": [
    "### **1.0** Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Lf8ufHH5JrFu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text                date  \n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  December 31, 2017   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  December 29, 2017   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  December 31, 2017   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  December 30, 2017   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  December 29, 2017   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the DataFrame with True News to understand the given data\n",
    "True_News.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gI7X0Voh6h7r"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text               date  \n",
       "0  Donald Trump just couldn t wish all Americans ...  December 31, 2017  \n",
       "1  House Intelligence Committee Chairman Devin Nu...  December 31, 2017  \n",
       "2  On Friday, it was revealed that former Milwauk...  December 30, 2017  \n",
       "3  On Christmas day, Donald Trump announced that ...  December 29, 2017  \n",
       "4  Pope Francis used his annual Christmas Day mes...  December 25, 2017  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the DataFrame with Fake News to understand the given data\n",
    "Fake_News.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Dwcty-wmJrFw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21417 entries, 0 to 21416\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   21417 non-null  object\n",
      " 1   text    21417 non-null  object\n",
      " 2   date    21417 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 502.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the column details for True News DataFrame\n",
    "True_News.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EPLAnUMjjzQ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23523 entries, 0 to 23522\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   23502 non-null  object\n",
      " 1   text    23502 non-null  object\n",
      " 2   date    23481 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 551.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the column details for Fake News Dataframe\n",
    "Fake_News.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dyuDFzPkI67B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in True_News:\n",
      "Index(['title', 'text', 'date'], dtype='object')\n",
      "\n",
      "Columns in Fake_News:\n",
      "Index(['title', 'text', 'date'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the column names of both DataFrames\n",
    "# Print column names of True_News\n",
    "print(\"Columns in True_News:\")\n",
    "print(True_News.columns)\n",
    "\n",
    "# Print column names of Fake_News\n",
    "print(\"\\nColumns in Fake_News:\")\n",
    "print(Fake_News.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2fff1S7hq5h"
   },
   "source": [
    "### **1.1** Add new column  <font color = red>[3 marks]</font> <br>\n",
    "\n",
    "Add new column `news_label` to both the DataFrames and assign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YOu_KhbH78du"
   },
   "outputs": [],
   "source": [
    "# Add a new column 'news_label' to the true news DataFrame and assign the label \"1\" to indicate that these news are true\n",
    "True_News[\"news_label\"] = 1 \n",
    "# Add a new column 'news_label' to the fake news DataFrame and assign the label \"0\" to indicate that these news are fake\n",
    "Fake_News[\"news_label\"] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UShuo3h54DAh"
   },
   "source": [
    "### **1.2** Merge DataFrames  <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Create a new Dataframe by merging True and Fake DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5GX_QMy04M4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined dataset: (44940, 4)\n"
     ]
    }
   ],
   "source": [
    "# Combine the true and fake news DataFrames into a single DataFrame\n",
    "News_Data = pd.concat([True_News,Fake_News], axis=0, ignore_index=True)\n",
    "# Show shape of the combined DataFrame\n",
    "print(\"Shape of combined dataset:\", News_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FYCtKXD1JrFw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>news_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  December 31, 2017    \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  December 29, 2017    \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  December 31, 2017    \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  December 30, 2017    \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  December 29, 2017    \n",
       "\n",
       "   news_label  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the combined DataFrame to verify the result\n",
    "News_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IphUaBu3oFZK"
   },
   "source": [
    "### **1.3** Handle the null values  <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Check for null values and handle it by imputation or dropping the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4FRDxOo6r51j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         21\n",
       "text          21\n",
       "date          42\n",
       "news_label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Presence of Null Values\n",
    "News_Data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eiwvoyfYr6EB"
   },
   "outputs": [],
   "source": [
    "# Handle Rows with Null Values\n",
    "News_Data = News_Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         0\n",
       "text          0\n",
       "date          0\n",
       "news_label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify again\n",
    "News_Data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDyPvMITooWA"
   },
   "source": [
    "### **1.4** Merge the relevant columns and drop the rest from the DataFrame  <font color = red>[3 marks]</font> <br>\n",
    "\n",
    "Combine the relevant columns into a new column `news_text` and then drop irrelevant columns from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u9VI48jS_HTy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>news_label</th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>1</td>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>1</td>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>1</td>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>1</td>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text  news_label  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...           1   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...           1   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...           1   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...           1   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...           1   \n",
       "\n",
       "                                           news_text  \n",
       "0  As U.S. budget fight looms, Republicans flip t...  \n",
       "1  U.S. military to accept transgender recruits o...  \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...  \n",
       "3  FBI Russia probe helped by Australian diplomat...  \n",
       "4  Trump wants Postal Service to charge 'much mor...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the relevant columns into a new column 'news_text' by joining their values with a space\n",
    "News_Data[\"news_text\"] = News_Data[\"title\"].astype(str) + \" \" + News_Data[\"text\"].astype(str)\n",
    "# Drop the irrelevant columns from the DataFrame as they are no longer needed\n",
    "News_Data = News_Data.drop(columns=[ \"date\"], errors=\"ignore\")\n",
    "\n",
    "# Display the first 5 rows of the updated DataFrame to check the result\n",
    "News_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L944HZpsJrFy"
   },
   "source": [
    "## **2.** Text Preprocessing <font color = red>[15 marks]</font> <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On all the news text, you need to:\n",
    "<ol type=1>\n",
    "  <li> Make the text lowercase\n",
    "  <li> Remove text in square brackets\n",
    "  <li> Remove punctuation\n",
    "  <li> Remove words containing numbers\n",
    "</ol>\n",
    "\n",
    "\n",
    "Once you have done these cleaning operations you need to perform POS tagging and lemmatization on the cleaned news text, and remove all words that are not tagged as NN or NNS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-6VW3V3jx1A"
   },
   "source": [
    "### **2.1** Text Cleaning  <font color = red>[5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78OZs7P4kp41"
   },
   "source": [
    "#### 2.1.0 Create a new DataFrame to store the processed data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uXnN7aa_JrF0"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame('df_clean') that will have only the cleaned news text and the lemmatized news text with POS tags removed\n",
    "df_clean = pd.DataFrame({\n",
    "    \"cleaned_text\": [\"\"]* len(News_Data),\n",
    "    \"lemmatized_text\": [\"\"]* len(News_Data)})\n",
    "\n",
    "# Add 'news_label' column to the new dataframe for topic identification\n",
    "\n",
    "df_clean[\"news_label\"] = News_Data[\"news_label\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsf00J83mdNL"
   },
   "source": [
    "#### 2.1.1 Write the function to clean the text and remove all the unnecessary elements  <font color = red>[4 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qm7SjjSkJrFz"
   },
   "outputs": [],
   "source": [
    "# Write the function here to clean the text and remove all the unnecessary elements\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # 1. Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove text in square brackets\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    \n",
    "    # 3. Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # 4. Remove words containing numbers\n",
    "    text = re.sub(r\"\\w*\\d\\w*\", \"\", text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDFMNnrdkUvd"
   },
   "source": [
    "#### 2.1.2  Apply the function to clean the news text and store the cleaned text in a new column within the new DataFrame. <font color = red>[1 mark]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nOiDVvEIJrF0"
   },
   "outputs": [],
   "source": [
    "# Apply the function to clean the news text and remove all unnecessary elements\n",
    "cleaned_output = News_Data[\"news_text\"].apply(clean_text)\n",
    "# Store it in a separate column in the new DataFrame\n",
    "df_clean[\"cleaned_text\"] = cleaned_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>news_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as us budget fight looms republicans flip thei...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us military to accept transgender recruits on ...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>senior us republican senator let mr mueller do...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbi russia probe helped by australian diplomat...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump wants postal service to charge much more...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text lemmatized_text  \\\n",
       "0  as us budget fight looms republicans flip thei...                   \n",
       "1  us military to accept transgender recruits on ...                   \n",
       "2  senior us republican senator let mr mueller do...                   \n",
       "3  fbi russia probe helped by australian diplomat...                   \n",
       "4  trump wants postal service to charge much more...                   \n",
       "\n",
       "   news_label  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSAVnSelkF9d"
   },
   "source": [
    "### **2.2** POS Tagging and Lemmatization  <font color = red>[10 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCqNHDL2mHok"
   },
   "source": [
    "#### 2.2.1 Write the function for POS tagging and lemmatization, filtering stopwords and keeping only NN and NNS tags <font color = red>[8 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zgOu8t8HJrFz"
   },
   "outputs": [],
   "source": [
    "# Write the function for POS tagging and lemmatization, filtering stopwords and keeping only NN and NNS tags\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure resources are downloaded (run once)\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_lemmatize_nouns(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())  # tokenize and lowercase\n",
    "    pos_tags = nltk.pos_tag(tokens)            # POS tagging\n",
    "\n",
    "    lemmas = []\n",
    "    for word, tag in pos_tags:\n",
    "        if word.isalpha() and word not in stop_words:   # keep only words, no digits/punct\n",
    "            if tag in [\"NN\", \"NNS\"]:  # only nouns\n",
    "                lemma = lemmatizer.lemmatize(word, wordnet.NOUN)\n",
    "                lemmas.append(lemma)\n",
    "    return \" \".join(lemmas)  # return as a single cleaned sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T3So7PXlibT"
   },
   "source": [
    "#### 2.2.2  Apply the POS tagging and lemmatization function to cleaned text and store it in a new column within the new DataFrame. <font color = red>[2 mark]</font> <br>\n",
    "\n",
    "**NOTE: Store the cleaned text and the lemmatized text with POS tags removed in separate columns within the new DataFrame.**\n",
    "\n",
    "**This will be useful for analysing character length differences between cleaned text and lemmatized text with POS tags removed during EDA.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "L9FWmibNI67F"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply POS tagging and lemmatization function to cleaned text\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#sample_text = df_clean.loc[0, \"cleaned_text\"]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#lemmatized_output1 = pos_lemmatize(sample_text)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m lemmatized_output \u001b[38;5;241m=\u001b[39m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(pos_lemmatize_nouns)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#print(lemmatized_output1)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Store it in a separate column in the new DataFrame\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatized_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lemmatized_output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m, in \u001b[0;36mpos_lemmatize_nouns\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_lemmatize_nouns\u001b[39m(text):\n\u001b[0;32m     16\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text\u001b[38;5;241m.\u001b[39mlower())  \u001b[38;5;66;03m# tokenize and lowercase\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     pos_tags \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(tokens)            \u001b[38;5;66;03m# POS tagging\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m pos_tags:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:169\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    168\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:126\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:201\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[0;32m    200\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[1;32m--> 201\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(features, return_conf)\n\u001b[0;32m    202\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[0;32m    204\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:86\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[1;34m(self, features, return_conf)\u001b[0m\n\u001b[0;32m     83\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# compute the confidence\u001b[39;00m\n\u001b[0;32m     88\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_softmax(scores)) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:86\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict.<locals>.<lambda>\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m     83\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# compute the confidence\u001b[39;00m\n\u001b[0;32m     88\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_softmax(scores)) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply POS tagging and lemmatization function to cleaned text\n",
    "#sample_text = df_clean.loc[0, \"cleaned_text\"]\n",
    "#lemmatized_output1 = pos_lemmatize(sample_text)\n",
    "df_clean[\"cleaned_text\"] = df_clean[\"cleaned_text\"].fillna(\"\").astype(str)\n",
    "\n",
    "lemmatized_output = df_clean[\"cleaned_text\"].apply(pos_lemmatize_nouns)\n",
    "#print(lemmatized_output1)\n",
    "# Store it in a separate column in the new DataFrame\n",
    "df_clean[\"lemmatized_text\"] = lemmatized_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMc5kjeqnX9b"
   },
   "source": [
    "### Save the Cleaned data as a csv file (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTjNG5xfnlwm"
   },
   "outputs": [],
   "source": [
    "## Recommended to perform the below steps to save time while rerunning the code\n",
    "# df_clean.to_csv(\"clean_df.csv\", index=False)\n",
    "df_clean.to_csv(\"clean_df.csv\", index=False)\n",
    "# df_clean = pd.read_csv(\"clean_df.csv\")\n",
    "df_clean = pd.read_csv(\"clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0M0LseVjneMv"
   },
   "outputs": [],
   "source": [
    "# Check the first few rows of the DataFrame\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDZq_T3FYuOi"
   },
   "outputs": [],
   "source": [
    "# Check the dimensions of the DataFrame\n",
    "print(\"Dimensions of df_clean:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah1aJPmiAqWz"
   },
   "outputs": [],
   "source": [
    "# Check the number of non-null entries and data types of each column\n",
    "print(df_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMzqKme_2QQ0"
   },
   "source": [
    "## **3.** Train Validation Split <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmx_W7Ty2PlK"
   },
   "outputs": [],
   "source": [
    "# Import Train Test Split and split the DataFrame into 70% train and 30% validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df_clean, \n",
    "    test_size=0.3,   # 30% validation\n",
    "    random_state=42, \n",
    "    stratify=df_clean[\"news_label\"] # keep label distribution balanced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Un1AElJrF2"
   },
   "source": [
    "## **4.** Exploratory Data Analysis on Training Data  <font color = red>[40 marks]</font> <br>\n",
    "\n",
    "Perform EDA on cleaned and preprocessed texts to get familiar with the training data by performing the tasks given below:\n",
    "\n",
    "<ul>\n",
    "  <li> Visualise the training data according to the character length of cleaned news text and lemmatized news text with POS tags removed\n",
    "  <li> Using a word cloud, find the top 40 words by frequency in true and fake news separately\n",
    "  <li> Find the top unigrams, bigrams and trigrams by frequency in true and fake news separately\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOZ7yZ4Fp1cp"
   },
   "source": [
    "### **4.1** Visualise character lengths of cleaned news text and lemmatized news text with POS tags removed  <font color = red>[10 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCI4DbZWpQ7n"
   },
   "source": [
    "##### 4.1.1  Add new columns to calculate the character lengths of the processed data columns  <font color = red>[3 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HifHm5rxpaxZ"
   },
   "outputs": [],
   "source": [
    "# Add a new column to calculate the character length of cleaned news text\n",
    "df_clean[\"char_length\"] = df_clean[\"cleaned_text\"].str.len()\n",
    "\n",
    "# Add a new column to calculate the character length of lemmatized news text with POS tags removed\n",
    "df_clean[\"lemm_char_length\"] = df_clean[\"lemmatized_text\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1JvT8lNpbFe"
   },
   "source": [
    "##### 4.1.2  Create Histogram to visualise character lengths  <font color = red>[7 marks]</font> <br>\n",
    "\n",
    " Plot both distributions on the same graph for comparison and to observe overlaps and peak differences to understand text preprocessing's impact on text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-zaqJF6JrF2"
   },
   "outputs": [],
   "source": [
    "# Create a histogram plot to visualise character lengths\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.hist(df_clean[\"char_length\"], bins=50, alpha=0.6, label=\"Cleaned Text\")\n",
    "plt.hist(df_clean[\"lemm_char_length\"], bins=50, alpha=0.6, label=\"Lemmatized Text\")\n",
    "\n",
    "plt.title(\"Histogram of Character Lengths\", fontsize=14)\n",
    "plt.xlabel(\"Character Length\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Add histogram for cleaned news text\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df_clean[\"char_length\"], bins=50, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Histogram of Character Lengths - Cleaned News Text\", fontsize=14)\n",
    "plt.xlabel(\"Character Length\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.show()\n",
    "# Add histogram for lemmatized news text with POS tags removed\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df_clean[\"lemmatized_char_length\"], bins=50, color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Histogram of Character Lengths - Lemmatized News Text\", fontsize=14)\n",
    "plt.xlabel(\"Character Length\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9jD_6SeJrF3"
   },
   "source": [
    "### **4.2** Find and display the top 40 words by frequency among true and fake news in Training data after processing the text  <font color = red>[10 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n320yzDiEUH4"
   },
   "source": [
    "##### 4.2.1 Find and display the top 40 words by frequency among true news in Training data after processing the text  <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcfdvtfZJrF3"
   },
   "outputs": [],
   "source": [
    "## Use a word cloud find the top 40 words by frequency among true news in the training data after processing the text\n",
    "from wordcloud import WordCloud\n",
    "# Filter news with label 1 (True News) and convert to it string and handle any non-string values\n",
    "true_train = train_df[train_df[\"news_label\"] == 1]\n",
    "true_train_text = \" \".join(true_train[\"lemmatized_text\"].dropna().astype(str))\n",
    "\n",
    "# Generate word cloud for True News\n",
    "wordcloud_true = WordCloud(width=1000,\n",
    "                      height=600,\n",
    "                      background_color=\"white\",\n",
    "                      max_words=40,\n",
    "                      colormap=\"viridis\").generate(true_train_text)\n",
    "\n",
    "# Plot word cloud\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Top 40 Words in True News (After Processing)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbHtjWzbEj__"
   },
   "source": [
    "##### 4.2.2 Find and display the top 40 words by frequency among fake news in Training data after processing the text  <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOFoDEscEkMO"
   },
   "outputs": [],
   "source": [
    "## Use a word cloud find the top 40 words by frequency among fake news in the training data after processing the text\n",
    "\n",
    "# Filter news with label 0 (Fake News) and convert to it string and handle any non-string values\n",
    "fake_train = train_df[train_df[\"news_label\"] == 0]\n",
    "fake_train_text = \" \".join(fake_train[\"lemmatized_text\"].dropna().astype(str))\n",
    "\n",
    "# Generate word cloud for Fake News\n",
    "wordcloud_fake = WordCloud(width=1000,\n",
    "                           height=600,\n",
    "                           background_color=\"white\",\n",
    "                           max_words=40,\n",
    "                           colormap=\"magma\").generate(fake_train_text)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud_fake, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Top 40 Words in Fake News (After Processing)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DfCSbbmJrF4"
   },
   "source": [
    "### **4.3** Find and display the top unigrams, bigrams and trigrams by frequency in true news and fake news after processing the text  <font color = red>[20 marks]</font> <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApGagSppsAyL"
   },
   "source": [
    "##### 4.3.1 Write a function to get the specified top n-grams  <font color = red>[4 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mbk5DS5JrF4"
   },
   "outputs": [],
   "source": [
    "# Write a function to get the specified top n-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_top_ngrams(corpus, n=20, ngram_range=(1,2)):\n",
    "    # If input is a single string, convert to list\n",
    "    if isinstance(corpus, str):\n",
    "        corpus = [corpus]\n",
    "    \n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Sum up counts of each n-gram\n",
    "    freqs = X.sum(axis=0)\n",
    "    freqs = freqs.A1  # convert to 1D array\n",
    "    \n",
    "    # Map n-grams to their frequencies\n",
    "    ngrams_freq = dict(zip(vectorizer.get_feature_names_out(), freqs))\n",
    "    \n",
    "    # Create DataFrame sorted by frequency\n",
    "    top_ngrams_df = pd.DataFrame(ngrams_freq.items(), columns=[\"ngram\", \"frequency\"])\n",
    "    top_ngrams_df = top_ngrams_df.sort_values(by=\"frequency\", ascending=False).head(n)\n",
    "    \n",
    "    return top_ngrams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHcBL7vRsM4I"
   },
   "source": [
    "##### 4.3.2 Handle the NaN values  <font color = red>[1 mark]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ks69UQGCXpw"
   },
   "outputs": [],
   "source": [
    "# Handle NaN values in the text data\n",
    "# Replace NaN in 'lemmatized_text' with empty string\n",
    "df_clean['lemmatized_text'] = df_clean['lemmatized_text'].fillna(\"missing_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caioIgIEsfh2"
   },
   "source": [
    "### For True News\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYB2ZXZ83fjo"
   },
   "source": [
    "##### 4.3.3 Display the top 10 unigrams by frequency in true news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX7fedm1JrF8"
   },
   "outputs": [],
   "source": [
    "# Print the top 10 unigrams by frequency in true news and plot the same using a bar graph\n",
    "top_true_unigrams = get_top_ngrams(true_train_text, n=10, ngram_range=(1,1))\n",
    "print(top_true_unigrams)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(top_true_unigrams[\"ngram\"], top_true_unigrams[\"frequency\"], color=\"skyblue\")\n",
    "plt.title(\"Top 10 Unigrams in True News (Training Data)\", fontsize=14)\n",
    "plt.xlabel(\"Unigrams\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3Q7wQnnsvOE"
   },
   "source": [
    "##### 4.3.4 Display the top 10 bigrams by frequency in true news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aV7kD7w8JrF8"
   },
   "outputs": [],
   "source": [
    "# Print the top 10 bigrams by frequency in true news and plot the same using a bar graph\n",
    "top_true_bigrams = get_top_ngrams(true_train_text, n=10, ngram_range=(2,2))\n",
    "print(top_true_bigrams)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(top_true_bigrams[\"ngram\"], top_true_bigrams[\"frequency\"], color=\"lightgreen\")\n",
    "plt.title(\"Top 10 Bigrams in True News (Training Data)\", fontsize=14)\n",
    "plt.xlabel(\"Bigrams\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opdM04-Bs_Bg"
   },
   "source": [
    "##### 4.3.5 Display the top 10 trigrams by frequency in true news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xkh7vtbtJrF-"
   },
   "outputs": [],
   "source": [
    "# Print the top 10 trigrams by frequency in true news and plot the same using a bar graph\n",
    "top_true_trigrams = get_top_ngrams(true_train_text, n=10, ngram_range=(3,3))\n",
    "print(top_true_trigrams)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(top_true_trigrams[\"ngram\"], top_true_trigrams[\"frequency\"], color=\"salmon\")\n",
    "plt.title(\"Top 10 Trigrams in True News (Training Data)\", fontsize=14)\n",
    "plt.xlabel(\"Trigrams\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prNx2Sm0WGEj"
   },
   "source": [
    "### For Fake News\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obBKlBVK3mfX"
   },
   "source": [
    "##### 4.3.6 Display the top 10 unigrams by frequency in fake news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qKDzoSIWGXp"
   },
   "outputs": [],
   "source": [
    "# Print the top 10 unigrams by frequency in fake news and plot the same using a bar graph\n",
    "top_fake_unigrams = get_top_ngrams(fake_train_text, n=10, ngram_range=(1,1))\n",
    "print(top_fake_unigrams)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(top_fake_unigrams[\"ngram\"], top_fake_unigrams[\"frequency\"], color=\"orange\")\n",
    "plt.title(\"Top 10 Unigrams in Fake News (Training Data)\", fontsize=14)\n",
    "plt.xlabel(\"Unigrams\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSlMnmqcYsNw"
   },
   "source": [
    "##### 4.3.7 Display the top 10 bigrams by frequency in fake news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5xUk_r9Wa0f"
   },
   "outputs": [],
   "source": [
    "# Print the top 10 bigrams by frequency in fake news and plot the same using a bar graph\n",
    "top_fake_bigrams = get_top_ngrams(fake_train_text, n=10, ngram_range=(2,2))\n",
    "print(top_fake_bigrams)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(top_fake_bigrams[\"ngram\"], top_fake_bigrams[\"frequency\"], color=\"lightcoral\")\n",
    "plt.title(\"Top 10 Bigrams in Fake News (Training Data)\", fontsize=14)\n",
    "plt.xlabel(\"Bigrams\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i08WYIwPYs6R"
   },
   "source": [
    "##### 4.3.8 Display the top 10 trigrams by frequency in fake news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nT9a1EvWa-s"
   },
   "outputs": [],
   "source": [
    "# Print the top 10 trigrams by frequency in fake news and plot the same using a bar graph\n",
    "top_fake_trigrams = get_top_ngrams(fake_train_text, n=10, ngram_range=(3,3))\n",
    "print(top_fake_trigrams)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(top_fake_trigrams[\"ngram\"], top_fake_trigrams[\"frequency\"], color=\"mediumseagreen\")\n",
    "plt.title(\"Top 10 Trigrams in Fake News (Training Data)\", fontsize=14)\n",
    "plt.xlabel(\"Trigrams\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VsOO_oHN8-_"
   },
   "source": [
    "## **5.** Exploratory Data Analysis on Validation Data [Optional]\n",
    "\n",
    "Perform EDA on validation data to differentiate EDA on training data with EDA on validation data and the tasks are given below:\n",
    "\n",
    "<ul>\n",
    "  <li> Visualise the data according to the character length of cleaned news text and lemmatized text with POS tags removed\n",
    "  <li> Using a word cloud find the top 40 words by frequency in true and fake news separately\n",
    "  <li> Find the top unigrams, bigrams and trigrams by frequency in true and fake news separately\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNkif5wrONfI"
   },
   "source": [
    "### **5.1** Visualise character lengths of cleaned news text and lemmatized news text with POS tags removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAhwSWBWOTIj"
   },
   "source": [
    "##### 5.1.1  Add new columns to calculate the character lengths of the processed data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hYYTrdHORAs"
   },
   "outputs": [],
   "source": [
    "# Add a new column to calculate the character length of cleaned news text\n",
    "\n",
    "# Add a new column to calculate the character length of lemmatized news text with POS tags removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRILiD8hOX5q"
   },
   "source": [
    "##### 5.1.2  Create Histogram to visualise character lengths\n",
    "\n",
    "Plot both distributions on the same graph for comparison and to observe overlaps and peak differences to understand text preprocessing's impact on text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7LLOXC0Oaix"
   },
   "outputs": [],
   "source": [
    "# Create a histogram plot to visualise character lengths\n",
    "\n",
    "# Add histogram for cleaned news text\n",
    "\n",
    "# Add histogram for lemmatized news text with POS tags removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPxa0JrvOwFu"
   },
   "source": [
    "### **5.2** Find and display the top 40 words by frequency among true and fake news after processing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOZRD8vaFt2h"
   },
   "source": [
    "##### 5.2.1  Find and display the top 40 words by frequency among true news in validation data after processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_9UVs4WOw0_"
   },
   "outputs": [],
   "source": [
    "## Use a word cloud find the top 40 words by frequency among true news after processing the text\n",
    "\n",
    "# Generate word cloud for True News\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6AQPITsFuop"
   },
   "source": [
    "##### 5.2.2  Find and display the top 40 words by frequency among fake news in validation data after processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gk6xTdSFu1X"
   },
   "outputs": [],
   "source": [
    "## Use a word cloud find the top 40 words by frequency among fake news after processing the text\n",
    "\n",
    "# Generate word cloud for Fake News\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx0s2a7xOxKf"
   },
   "source": [
    "### **5.3** Find and display the top unigrams, bigrams and trigrams by frequency in true news and fake news after processing the text  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2xZKU0RO5ft"
   },
   "source": [
    "##### 5.3.1 Write a function to get the specified top n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g70ZFxcoOxS2"
   },
   "outputs": [],
   "source": [
    "## Write a function to get the specified top n-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8ud5-r7O7ut"
   },
   "source": [
    "##### 5.3.2 Handle the NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsBu-aotPBJF"
   },
   "outputs": [],
   "source": [
    "## First handle NaN values in the text data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KVWcxoDPAiE"
   },
   "source": [
    "### For True News\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am3uuIlU4wj1"
   },
   "source": [
    "\n",
    "##### 5.3.3 Display the top 10 unigrams by frequency in true news and plot them as a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKdpz-XmPGHD"
   },
   "outputs": [],
   "source": [
    "## Print the top 10 unigrams by frequency in true news and plot the same using a bar graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiKrOL8rPLAs"
   },
   "source": [
    "##### 5.3.4 Display the top 10 bigrams by frequency in true news and plot them as a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuyYGnaNPLwq"
   },
   "outputs": [],
   "source": [
    "## Print the top 10 bigrams by frequency in true news and plot the same using a bar graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f8_h-BiPOKb"
   },
   "source": [
    "##### 5.3.5 Display the top 10 trigrams by frequency in true news and plot them as a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lz-XS7qPQZj"
   },
   "outputs": [],
   "source": [
    "## Print the top 10 trigrams by frequency in true news and plot the same using a bar graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MfApeGrd6Fl"
   },
   "source": [
    "### For Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CH1csmZeGqh"
   },
   "source": [
    "##### 5.3.6 Display the top 10 unigrams by frequency in fake news and plot them as a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w310WzGAeG4K"
   },
   "outputs": [],
   "source": [
    "## Print the top 10 unigrams by frequency in fake news and plot the same using a bar graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFjOaPx7eHFw"
   },
   "source": [
    "##### 5.3.7 Display the top 10 bigrams by frequency in fake news and plot them as a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuTqnjkIeHSJ"
   },
   "outputs": [],
   "source": [
    "## Print the top 10 bigrams by frequency in fake news and plot the same using a bar graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn_oiixheHf_"
   },
   "source": [
    "##### 5.3.8 Display the top 10 trigrams by frequency in fake news and plot them as a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xduyO4gheHtI"
   },
   "outputs": [],
   "source": [
    "## Print the top 10 trigrams by frequency in fake news and plot the same using a bar graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-I0k0QtJrGA"
   },
   "source": [
    "## **6.** Feature Extraction  <font color = red>[10 marks]</font> <br>\n",
    "\n",
    "For any ML model to perform classification on textual data, you need to convert it to a vector form. In this assignment, you will use the Word2Vec Vectorizer to create vectors from textual data. Word2Vec model captures the semantic relationship between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09xy3mAbtZgZ"
   },
   "source": [
    "### **6.1** Initialise Word2Vec model  <font color = red>[2 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8fGwaCPJrGA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Write your code here to initialise the Word2Vec model by downloading \"word2vec-google-news-300\"\n",
    "import gensim.downloader as api\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYzD85nTJrGA"
   },
   "source": [
    "### **6.2** Extract vectors for cleaned news data   <font color = red>[8 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffzdDpp_JrGB"
   },
   "outputs": [],
   "source": [
    "## Write your code here to extract the vectors from the Word2Vec model for both training and validation data\n",
    "import numpy as np\n",
    "def get_doc_vector(text, model):\n",
    "    words = text.split()\n",
    "    word_vecs = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            word_vecs.append(model[word])\n",
    "    \n",
    "    if len(word_vecs) > 0:\n",
    "        return np.mean(word_vecs, axis=0)\n",
    "    else:\n",
    "        # If none of the words exist in the model, return zero vector\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# ----------------------------\n",
    "# Training data vectors\n",
    "# ----------------------------\n",
    "X_train_vectors = np.array([get_doc_vector(text, word2vec_model) for text in train_df[\"lemmatized_text\"].astype(str)])\n",
    "\n",
    "# ----------------------------\n",
    "# Validation data vectors\n",
    "# ----------------------------\n",
    "X_val_vectors = np.array([get_doc_vector(text, word2vec_model) for text in val_df[\"lemmatized_text\"].astype(str)])\n",
    "\n",
    "\n",
    "print(\"Training vectors shape:\", X_train_vectors.shape)\n",
    "print(\"Validation vectors shape:\", X_val_vectors.shape)\n",
    "\n",
    "\n",
    "## Extract the target variable for the training data and validation data\n",
    "y_train = train_df[\"news_label\"].values\n",
    "y_val = val_df[\"news_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q9lwvNEJrGB"
   },
   "source": [
    "## **7.** Model Training and Evaluation <font color = red>[45 marks]</font>\n",
    "\n",
    "You will use a set of supervised models to classify the news into true or fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO16REK-xpq4"
   },
   "source": [
    "### **7.0** Import models and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0BKoT3wxpq4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLuHH1olZXQq"
   },
   "source": [
    "### **7.1** Build Logistic Regression Model  <font color = red>[15 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1ItmvvGwduv"
   },
   "source": [
    "##### 7.1.1 Create and train logistic regression model on training data  <font color = red>[10 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzXd3YSu5eyY"
   },
   "outputs": [],
   "source": [
    "## Initialise Logistic Regression model\n",
    "logreg_model = LogisticRegression(\n",
    "    solver='liblinear',   # good for binary classification\n",
    "    random_state=42\n",
    ")\n",
    "## Train Logistic Regression model on training data\n",
    "logreg_model.fit(X_train_vectors, y_train)\n",
    "## Predict on validation data\n",
    "y_val_pred = logreg_model.predict(X_val_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvNKC8ob8IAL"
   },
   "source": [
    "##### 7.1.2 Calculate and print accuracy, precision, recall and f1-score on validation data <font color = red>[5 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEyQcSoWo4xs"
   },
   "outputs": [],
   "source": [
    "## Calculate and print accuracy, precision, recall, f1-score on predicted labels\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_val, y_val_pred)\n",
    "\n",
    "# F1-score\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Validation Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {precision:.4f}\")\n",
    "print(f\"Validation Recall   : {recall:.4f}\")\n",
    "print(f\"Validation F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6P4MA_AC216"
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "report = classification_report(y_val, y_val_pred, target_names=[\"Fake\", \"True\"])\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRGPMQZd8r8B"
   },
   "source": [
    "### **7.2** Build Decision Tree Model <font color = red>[15 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rgv4vqrt81sH"
   },
   "source": [
    "##### 7.2.1 Create and train a decision tree model on training data <font color = red>[10 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-mTab94xpq4"
   },
   "outputs": [],
   "source": [
    "## Initialise Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    criterion='gini',       # 'gini' or 'entropy'\n",
    "    max_depth=None,         # You can set a max depth if needed\n",
    "    random_state=42\n",
    ")\n",
    "## Train Decision Tree model on training data\n",
    "dt_model.fit(X_train_vectors, y_train)\n",
    "\n",
    "## Predict on validation data\n",
    "y_val_pred_dt = dt_model.predict(X_val_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ_Vj5fs6w9I"
   },
   "source": [
    "##### 7.2.2 Calculate and print accuracy, precision, recall and f1-score on validation data <font color = red>[5 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15iYiCQhp7jo"
   },
   "outputs": [],
   "source": [
    "## Calculate and print accuracy, precision, recall, f1-score on predicted labels\n",
    "\n",
    "# Accuracy\n",
    "accuracy_dt = accuracy_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# Precision\n",
    "precision_dt = precision_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# Recall\n",
    "recall_dt = recall_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# F1-score\n",
    "f1_dt = f1_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Decision Tree Validation Accuracy : {accuracy_dt:.4f}\")\n",
    "print(f\"Decision Tree Validation Precision: {precision_dt:.4f}\")\n",
    "print(f\"Decision Tree Validation Recall   : {recall_dt:.4f}\")\n",
    "print(f\"Decision Tree Validation F1-score : {f1_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DohNckxxxpq4"
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "report_dt = classification_report(y_val, y_val_pred_dt, target_names=[\"Fake\", \"True\"])\n",
    "print(\"Decision Tree Classification Report:\\n\")\n",
    "print(report_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnB_P9kd9EdC"
   },
   "source": [
    "### **7.3** Build Random Forest Model <font color = red>[15 marks]</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhW0nyU29In9"
   },
   "source": [
    "##### 7.3.1 Create and train a random forest model on training data <font color = red>[10 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIvY9-oPxpq4"
   },
   "outputs": [],
   "source": [
    "## Initialise Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,       # Number of trees in the forest\n",
    "    criterion='gini',       # 'gini' or 'entropy'\n",
    "    max_depth=None,         # You can set a max depth if needed\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # Use all available cores for faster training\n",
    ")\n",
    "## Train Random Forest model on training data\n",
    "rf_model.fit(X_train_vectors, y_train)\n",
    "## Predict on validation data\n",
    "y_val_pred_rf = rf_model.predict(X_val_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRBxZieM7eea"
   },
   "source": [
    " ##### 7.3.2 Calculate and print accuracy, precision, recall and f1-score on validation data <font color = red>[5 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzNK3jcCq01-"
   },
   "outputs": [],
   "source": [
    "## Calculate and print accuracy, precision, recall, f1-score on predicted labels\n",
    "# Accuracy\n",
    "accuracy_rf = accuracy_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Precision\n",
    "precision_rf = precision_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Recall\n",
    "recall_rf = recall_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# F1-score\n",
    "f1_rf = f1_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Random Forest Validation Accuracy : {accuracy_rf:.4f}\")\n",
    "print(f\"Random Forest Validation Precision: {precision_rf:.4f}\")\n",
    "print(f\"Random Forest Validation Recall   : {recall_rf:.4f}\")\n",
    "print(f\"Random Forest Validation F1-score : {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIBCo_kFxpq4"
   },
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "report_rf = classification_report(y_val, y_val_pred_rf, target_names=[\"Fake\", \"True\"])\n",
    "print(\"Random Forest Classification Report:\\n\")\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lnj7RUDDSEJX"
   },
   "source": [
    "## **8.** Conclusion <font color = red>[5 marks]</font>\n",
    "\n",
    "Summarise your findings by discussing patterns observed in true and fake news and how semantic classification addressed the problem. Highlight the best model chosen, the evaluation metric prioritised for the decision, and assess the approach and its impact."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
